{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6bd76c",
   "metadata": {},
   "source": [
    "# 3. Hierarchical Bayesian Logistic Regression (HLR)\n",
    "\n",
    "This section implements a hierarchical extension of the Bayesian logistic regression model on the German credit dataset. We introduce two-way interaction terms to expand our feature space to 300 dimensions. Additionally, the prior variance $\\sigma^2$ is now treated as an unknown parameter and given an exponential prior.\n",
    "\n",
    "To keep the parameter space unconstrained for HMC/NUTS, we sample $v = \\log(\\sigma^2)$ instead of $\\sigma^2$. The log-posterior is adjusted with the Jacobian of this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c183e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from bml.samplers import nuts, hmc\n",
    "from bml import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400016de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the German Credit (numeric) dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric\"\n",
    "data = pd.read_csv(url, sep=r'\\s+', header=None)\n",
    "\n",
    "X_raw = data.iloc[:, :-1].values\n",
    "y_raw = data.iloc[:, -1].values\n",
    "\n",
    "# Map y from {1, 2} to {1, -1}\n",
    "y = np.where(y_raw == 1, 1, -1)\n",
    "\n",
    "# Normalize original features to zero mean and unit variance\n",
    "X_norm = (X_raw - np.mean(X_raw, axis=0)) / np.std(X_raw, axis=0)\n",
    "\n",
    "# Expand predictors to include 2-way interactions (24 -> 300 dimensions)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_expanded = poly.fit_transform(X_norm)\n",
    "\n",
    "# Prepend a column of ones for the intercept alpha\n",
    "X = np.hstack([np.ones((X_expanded.shape[0], 1)), X_expanded])\n",
    "N, d_coeffs = X.shape  # N=1000, d_coeffs=301 (alpha + 300 beta coefficients)\n",
    "\n",
    "# Total parameters d = 301 coefficients + 1 variance parameter\n",
    "d = d_coeffs + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfee3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.01  # Rate parameter for the exponential prior on sigma^2\n",
    "\n",
    "def log_p(theta):\n",
    "    \"\"\"\n",
    "    Evaluates the log-posterior for Hierarchical Bayesian Logistic Regression.\n",
    "    theta[:301] = alpha and beta coefficients\n",
    "    theta[301] = v = log(sigma^2)\n",
    "    \"\"\"\n",
    "    if np.any(np.abs(theta) > 500.0) or theta[-1] < -20.0 or theta[-1] > 20.0:\n",
    "        return -np.inf\n",
    "    \n",
    "    coeffs = theta[:d_coeffs]\n",
    "    v = theta[-1]\n",
    "\n",
    "    sigma_sq = np.exp(v)\n",
    "    inv_sigma_sq = np.exp(-v)  # 1/sigma^2 for numerical stability\n",
    "\n",
    "    # Log-likelihood\n",
    "    z = -y * np.dot(X, coeffs)\n",
    "    log_likelihood = -np.sum(np.logaddexp(0, z))\n",
    "    \n",
    "    # Log-prior\n",
    "    log_prior = -0.5 * (np.sum(coeffs**2) * inv_sigma_sq) - (N / 2) * v - lam * sigma_sq + v\n",
    "    \n",
    "    return log_likelihood + log_prior\n",
    "\n",
    "def grad_log_p(theta):\n",
    "    \"\"\"\n",
    "    Evaluates the gradient of the log-posterior for HLR.\n",
    "    \"\"\"\n",
    "    theta_safe = np.clip(theta, -1000.0, 1000.0)\n",
    "    theta_safe[-1] = np.clip(theta_safe[-1], -20.0, 20.0)\n",
    "    \n",
    "    coeffs = theta_safe[:d_coeffs]\n",
    "    v = theta_safe[-1]\n",
    "\n",
    "    sigma_sq = np.exp(v)\n",
    "    inv_sigma_sq = np.exp(-v)  # 1/sigma^2 for numerical stability\n",
    "    grad = np.zeros_like(theta)\n",
    "    \n",
    "    # Gradient\n",
    "    z = -y * np.dot(X, coeffs)\n",
    "    s = expit(z)\n",
    "    grad_likelihood_coeffs = np.dot(X.T, y * s)\n",
    "    grad_prior_coeffs = -coeffs * inv_sigma_sq\n",
    "    \n",
    "    grad[:d_coeffs] = grad_likelihood_coeffs + grad_prior_coeffs\n",
    "    \n",
    "    # Gradient w.r.t. v = log(sigma^2)\n",
    "    grad[-1] = 0.5 * np.sum(coeffs**2) * inv_sigma_sq - (N / 2.0) - lam * sigma_sq + 1.0\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07b94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dual Averaging NUTS sampling for Hierarchical LR...\n",
      "Finished sampling with Dual Averaging NUTS. Output shape: (2001, 302)\n",
      "Worst-case ESS across all dimensions for Dual Averaging NUTS: 3.1770\n",
      "\n",
      "Starting Dual Averaging HMC sampling for Hierarchical LR...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m sampler = SamplerClass(L=log_p, grad=grad_log_p)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampler_name == \u001b[33m\"\u001b[39m\u001b[33mDual Averaging HMC\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# The optimal lambda length for HLR in the paper is roughly 0.14\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     samples = \u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m=\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_adapt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mM_adapt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     22\u001b[39m     samples = sampler.sample(theta0, delta=delta, M=M, M_adapt=M_adapt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Bayesian Machine Learning\\MVA-BML\\src\\bml\\samplers\\hmc.py:34\u001b[39m, in \u001b[36mDualAveragingHMC.sample\u001b[39m\u001b[34m(self, theta0, delta, lam, M, M_adapt)\u001b[39m\n\u001b[32m     31\u001b[39m L_m = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mround\u001b[39m(lam / epsilon))\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, L_m+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     theta_tilde, r_tilde = \u001b[43mleapfrog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_tilde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_tilde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Compute acceptance ratio in log space to avoid overflow\u001b[39;00m\n\u001b[32m     37\u001b[39m log_ratio = (\u001b[38;5;28mself\u001b[39m.L(theta_tilde) - \u001b[32m0.5\u001b[39m * np.dot(r_tilde, r_tilde)) - (\u001b[38;5;28mself\u001b[39m.L(theta_prev) - \u001b[32m0.5\u001b[39m * np.dot(r0, r0))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Bayesian Machine Learning\\MVA-BML\\src\\bml\\samplers\\utils.py:7\u001b[39m, in \u001b[36mleapfrog\u001b[39m\u001b[34m(theta, r, epsilon, grad)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs a leapfrog step in Hamiltonian Monte Carlo.\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Update momentum by half step\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m r_tilde = r + (epsilon / \u001b[32m2\u001b[39m) * \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Update position by full step\u001b[39;00m\n\u001b[32m     10\u001b[39m theta_tilde = theta + epsilon * r_tilde\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mgrad_log_p\u001b[39m\u001b[34m(theta)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Gradient w.r.t. coefficients (alpha and beta)\u001b[39;00m\n\u001b[32m     43\u001b[39m z = -y * np.dot(X, coeffs)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m s = \u001b[43mexpit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m grad_likelihood_coeffs = np.dot(X.T, y * s)\n\u001b[32m     46\u001b[39m grad_prior_coeffs = -coeffs * inv_sigma_sq\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initial point\n",
    "theta0 = np.zeros(d)\n",
    "theta0[-1] = np.log(100.0) \n",
    "\n",
    "M = 2000             # Total iterations\n",
    "M_adapt = 1000       # Warmup/adaptation iterations\n",
    "delta = 0.65         # Target acceptance rate\n",
    "\n",
    "results_hlr = {}\n",
    "\n",
    "for sampler_name, SamplerClass in [(\"Dual Averaging NUTS\", nuts.DualAveragingNUTS), \n",
    "                                   (\"Dual Averaging HMC\", hmc.DualAveragingHMC)]:\n",
    "    print(f\"Starting {sampler_name} sampling for Hierarchical LR...\")\n",
    "    \n",
    "    sampler = SamplerClass(L=log_p, grad=grad_log_p)\n",
    "    \n",
    "    if sampler_name == \"Dual Averaging HMC\":\n",
    "        # The optimal lambda length for HLR in the paper is roughly 0.14\n",
    "        samples = sampler.sample(theta0, delta=delta, lam=0.14, M=M, M_adapt=M_adapt)\n",
    "    else:\n",
    "        samples = sampler.sample(theta0, delta=delta, M=M, M_adapt=M_adapt)\n",
    "        \n",
    "    print(f\"Finished sampling with {sampler_name}. Output shape: {samples.shape}\")\n",
    "    \n",
    "    valid_samples = samples[M_adapt:]\n",
    "    \n",
    "    # Calculate worst-case ESS across all 302 dimensions\n",
    "    sample_means = np.mean(valid_samples, axis=0)\n",
    "    sample_vars = np.var(valid_samples, axis=0)\n",
    "    \n",
    "    min_ess = float('inf')\n",
    "    for dim in range(d):\n",
    "        dim_samples = valid_samples[:, dim]\n",
    "        mu = sample_means[dim]\n",
    "        var = sample_vars[dim]\n",
    "        \n",
    "        ess_mean = metrics.compute_ess_1d(dim_samples, mu, var)\n",
    "        \n",
    "        moment_samples = (dim_samples - mu)**2\n",
    "        moment_mu = np.mean(moment_samples)\n",
    "        moment_var = np.var(moment_samples)\n",
    "        ess_variance = metrics.compute_ess_1d(moment_samples, moment_mu, moment_var)\n",
    "        \n",
    "        min_ess = min(min_ess, ess_mean, ess_variance)\n",
    "        \n",
    "    results_hlr[sampler_name] = min_ess\n",
    "    print(f\"Worst-case ESS across all dimensions for {sampler_name}: {min_ess:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
